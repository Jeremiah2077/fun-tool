# ðŸŽ¯ FunGen Live Screen

> Advanced AI-Powered Real-Time Video Motion Analysis & Object Tracking Framework

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-green.svg)](https://opencv.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![YOLO](https://img.shields.io/badge/YOLO-v8-orange.svg)](https://github.com/ultralytics/ultralytics)

## ðŸ“‹ Overview

**FunGen Live Screen** is a cutting-edge computer vision framework designed for real-time video motion detection, multi-stage object tracking, and temporal pattern analysis. Built on state-of-the-art deep learning architectures and advanced signal processing algorithms, this platform provides professional-grade capabilities for video content analysis, motion intelligence extraction, and automated behavior recognition.

### Research & Industrial Applications

- ðŸ”¬ **Motion Research**: Quantitative analysis of periodic motion patterns and oscillation dynamics
- ðŸŽ¬ **Video Analytics**: Automated content segmentation, scene understanding, and temporal event detection
- ðŸŽ¯ **Object Tracking**: Multi-object tracking with contact analysis and spatial relationship mapping
- ðŸ“Š **Signal Intelligence**: Real-time frequency analysis, amplitude extraction, and trajectory reconstruction
- ðŸŽ¥ **VR/AR Processing**: Stereoscopic video format detection and GPU-accelerated frame processing
- ðŸ“ˆ **Data Visualization**: Real-time 3D simulation and multi-channel signal plotting

---

## âœ¨ Core Features

### ðŸ” **Multi-Stage Detection Pipeline**

Our proprietary detection pipeline employs a three-stage architecture:

- **Stage 1**: YOLO-based object detection with adaptive ROI (Region of Interest) extraction
- **Stage 2**: SQLite-backed contact analysis and temporal frame storage
- **Stage 3**: Hybrid processing combining optical flow and contact-based tracking

### ðŸŽ¯ **Advanced Tracking Modules**

Extensible tracker architecture with multiple algorithms:

- **Live Trackers**: Real-time oscillation detection, user-defined ROI tracking, YOLO-assisted analysis
- **Offline Trackers**: Frame-by-frame contact analysis, dense optical flow, hybrid mode processing
- **Experimental Modes**: Axis projection, beat detection, relative distance calculation, hybrid intelligence

### ðŸ“Š **Real-Time Visualization Suite**

- **Dynamic Graphs**: Multi-channel time-series plotting (frequency, position, rotation, intensity)
- **3D Motion Simulator**: OpenGL-based real-time trajectory visualization with dual-axis representation
- **Detection Overlay**: YOLO bounding box visualization with confidence scores

### ðŸ”§ **Signal Processing Plugin System**

Modular plugin architecture for advanced signal manipulation:

- **Amplification**: Dynamic signal amplification and amplitude adjustment
- **Filtering**: Savitzky-Golay smoothing, anti-jerk filtering, speed limiting
- **Optimization**: Ramer-Douglas-Peucker (RDP) curve simplification
- **Temporal**: Time-shifting, keyframe interpolation, resampling
- **AI-Powered**: Ultimate auto-tuning for optimal signal quality

### ðŸŽ¬ **VR/Stereoscopic Video Support**

- Machine learning-based VR format detection (Side-by-Side, Top-Bottom, Fisheye)
- GPU-accelerated video unwarping and perspective correction
- Dual-frame processing pipeline for stereoscopic content analysis

---

## ðŸš€ Quick Start

### Prerequisites

- **Python**: 3.8 or higher
- **GPU**: NVIDIA GPU with CUDA support (recommended for real-time performance)
- **RAM**: 8GB minimum, 16GB recommended
- **OS**: Windows 10/11, Linux (Ubuntu 20.04+)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/Jeremiah2077/fun-tool.git
cd fun-tool
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Install PyTorch with CUDA support** (for GPU acceleration)
```bash
# For CUDA 11.8
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# For CUDA 12.1
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

4. **Download YOLO model**
   - Place your trained YOLO model (`.pt` file) in the `models/` directory
   - Refer to `models/PLACE YOLO MODEL HERE` for specifications

### Basic Usage

**Screen Capture Mode** (Real-time):
```bash
python fungen_live.py
```

**Programmatic API**:
```python
from tracker.tracker_manager import TrackerManager
from funscript.dual_axis_funscript import DualAxisFunscript

# Initialize tracking engine
tracker = TrackerManager(
    model_path="models/your_model.pt",
    tracker_mode="yolo_roi",
    app=app_instance
)

# Process video frames
for frame in video_stream:
    processed_frame, tracking_data = tracker.process_frame(frame)

    # Extract motion metrics
    position = tracking_data.get('position', 0)
    frequency = tracking_data.get('frequency', 0)
    amplitude = tracking_data.get('amplitude', 0)
```

---

## ðŸ“ Project Architecture

```
fungen_live_screen/
â”‚
â”œâ”€â”€ common/                      # Shared utilities and managers
â”‚   â”œâ”€â”€ http_client_manager.py  # Async HTTP client
â”‚   â”œâ”€â”€ temp_manager.py          # Temporary file management
â”‚   â””â”€â”€ result.py                # Result data structures
â”‚
â”œâ”€â”€ config/                      # Configuration system
â”‚   â”œâ”€â”€ constants.py             # Application constants
â”‚   â”œâ”€â”€ theme_manager.py         # UI theme management
â”‚   â””â”€â”€ tracker_discovery.py    # Dynamic tracker loading
â”‚
â”œâ”€â”€ detection/                   # Multi-stage detection pipeline
â”‚   â””â”€â”€ cd/                      # Contact detection modules
â”‚       â”œâ”€â”€ stage_1_cd.py        # YOLO-based detection
â”‚       â”œâ”€â”€ stage_2_cd.py        # Contact analysis
â”‚       â”œâ”€â”€ stage_2_sqlite_storage.py  # Frame database
â”‚       â””â”€â”€ stage_3_*.py         # Hybrid processors
â”‚
â”œâ”€â”€ funscript/                   # Signal processing & export
â”‚   â”œâ”€â”€ dual_axis_funscript.py  # Dual-axis data structure
â”‚   â”œâ”€â”€ plugins/                 # Built-in processing plugins
â”‚   â”‚   â”œâ”€â”€ amplify_plugin.py
â”‚   â”‚   â”œâ”€â”€ savgol_filter_plugin.py
â”‚   â”‚   â”œâ”€â”€ rdp_simplify_plugin.py
â”‚   â”‚   â””â”€â”€ ultimate_autotune_plugin.py
â”‚   â””â”€â”€ user_plugins/            # Custom plugin directory
â”‚       â””â”€â”€ PLUGIN_DEVELOPMENT_GUIDE.md
â”‚
â”œâ”€â”€ models/                      # AI model storage
â”‚   â”œâ”€â”€ PLACE YOLO MODEL HERE
â”‚   â””â”€â”€ vr_detector_model_rf.pkl # VR format classifier
â”‚
â”œâ”€â”€ tracker/                     # Tracking engine
â”‚   â”œâ”€â”€ tracker_manager.py       # Main tracking orchestrator
â”‚   â””â”€â”€ tracker_modules/         # Modular tracker implementations
â”‚       â”œâ”€â”€ core/                # Base classes
â”‚       â”œâ”€â”€ live/                # Real-time trackers
â”‚       â”œâ”€â”€ offline/             # Batch processing trackers
â”‚       â”œâ”€â”€ experimental/        # Research trackers
â”‚       â””â”€â”€ templates/           # Developer templates
â”‚
â”œâ”€â”€ video/                       # Video processing utilities
â”‚   â”œâ”€â”€ video_processor.py       # Main video handler
â”‚   â”œâ”€â”€ vr_format_detector_ml_real.py  # ML-based VR detection
â”‚   â”œâ”€â”€ gpu_unwarp_worker.py     # GPU video correction
â”‚   â””â”€â”€ thumbnail_extractor.py   # Thumbnail generation
â”‚
â”œâ”€â”€ fungen_live.py               # Main screen capture application
â”œâ”€â”€ graph_display.py             # Real-time graph visualization
â”œâ”€â”€ simulator_3d.py              # 3D motion simulator
â””â”€â”€ requirements.txt             # Python dependencies
```

---

## ðŸ”§ Configuration & Modes

### Tracking Modes

| Mode | Description | Use Case |
|------|-------------|----------|
| `yolo_roi` | YOLO-assisted ROI detection | Object-based tracking with automatic region selection |
| `user_roi` | Manual ROI selection | Precise control over tracking region |
| `oscillation` | Automatic oscillation detection | Periodic motion pattern analysis |
| `optical_flow` | Dense optical flow tracking | High-precision motion field analysis |
| `contact_analysis` | Contact point detection | Interaction-based tracking |

### Output Formats

- **JSON**: Standard funscript format with temporal action points
- **SQLite**: Frame-level analysis database with metadata
- **CSV**: Statistical exports for external analysis tools

### Keyboard Controls (Live Mode)

| Key | Function |
|-----|----------|
| `S` | Start/Stop tracking |
| `R` | Reset tracking data |
| `G` | Toggle graph window |
| `D` | Toggle 3D simulator |
| `O` | Toggle info overlay |
| `Q/ESC` | Exit application |

---

## ðŸ› ï¸ Advanced Features

### Custom Plugin Development

Extend signal processing capabilities by creating custom plugins:

```python
from funscript.plugins.base_plugin import BasePlugin

class CustomProcessingPlugin(BasePlugin):
    """Custom signal processing plugin"""

    def __init__(self, app, **kwargs):
        super().__init__(app)
        self.parameter = kwargs.get('parameter', 1.0)

    def process(self, actions: list, **kwargs) -> list:
        """
        Process action points

        Args:
            actions: List of {'at': timestamp, 'pos': position} dicts

        Returns:
            Processed action list
        """
        processed = []
        for action in actions:
            # Apply custom processing logic
            new_pos = self._transform(action['pos'])
            processed.append({'at': action['at'], 'pos': new_pos})
        return processed

    def _transform(self, position: int) -> int:
        # Your transformation logic
        return int(position * self.parameter)
```

See `funscript/user_plugins/PLUGIN_DEVELOPMENT_GUIDE.md` for comprehensive documentation.

### Custom Tracker Implementation

Create specialized tracking algorithms:

```python
from tracker.tracker_modules.core.base_tracker import BaseTracker

class CustomTracker(BaseTracker):
    """Custom tracking algorithm implementation"""

    def __init__(self, app):
        super().__init__(app)
        self.name = "custom_tracker"
        self.description = "Custom motion tracking algorithm"

    def process(self, frame, detections, frame_info):
        """
        Process frame and extract tracking data

        Args:
            frame: numpy.ndarray (H, W, 3)
            detections: YOLO detection results
            frame_info: Frame metadata dict

        Returns:
            dict: {'position': int, 'rotation': float, ...}
        """
        # Implement your tracking logic
        position = self._calculate_position(frame, detections)
        rotation = self._calculate_rotation(frame, detections)

        return {
            'position': position,
            'rotation': rotation,
            'confidence': 0.95
        }
```

---

## ðŸ“Š Performance Optimization

### GPU Acceleration

The framework leverages GPU acceleration at multiple levels:

- **YOLO Inference**: CUDA-accelerated object detection (5-10x speedup)
- **Optical Flow**: GPU-based dense optical flow computation
- **Video Decoding**: Hardware-accelerated H.264/HEVC decoding
- **Frame Processing**: GPU unwarp workers for VR content

### Multi-Threading Architecture

- **Frame Capture**: Dedicated thread for screen/video capture
- **Detection Pipeline**: Parallel processing across stages
- **Storage**: Async SQLite writes with background workers
- **Visualization**: Separate rendering threads for UI responsiveness

### Benchmarks

| Configuration | FPS | Latency | GPU Usage |
|---------------|-----|---------|-----------|
| RTX 3060 + YOLO | 28-32 | <35ms | 45-60% |
| RTX 4070 + YOLO | 55-60 | <18ms | 30-40% |
| CPU Only (no YOLO) | 15-20 | <50ms | N/A |

---

## ðŸ”¬ Technical Specifications

### Supported Video Formats

**Standard Formats:**
- MP4 (H.264, H.265/HEVC)
- AVI (various codecs)
- MKV (Matroska)
- MOV (QuickTime)
- WebM (VP8, VP9)

**VR/Stereoscopic Formats:**
- Side-by-Side (SBS) - Half/Full
- Top-Bottom (TB) - Half/Full
- Equirectangular 360Â°
- Fisheye

**Live Sources:**
- Screen capture (MSS library)
- Webcam/USB cameras
- RTSP/RTMP streams
- Virtual cameras

### Signal Processing Specifications

- **Sampling Rate**: Up to 120 FPS
- **Position Resolution**: 0-100 (0.1 precision)
- **Rotation Range**: -90Â° to +90Â° (dual-axis)
- **Frequency Detection**: 0.1 - 10 Hz
- **Temporal Accuracy**: Â±5ms at 30 FPS

### System Requirements

| Component | Minimum | Recommended | Professional |
|-----------|---------|-------------|--------------|
| CPU | Intel i5-8400 / Ryzen 5 2600 | Intel i7-10700 / Ryzen 7 3700X | Intel i9-12900K / Ryzen 9 5950X |
| RAM | 8 GB | 16 GB | 32 GB |
| GPU | GTX 1060 6GB | RTX 3060 12GB | RTX 4070 Ti 12GB |
| Storage | 5 GB HDD | 20 GB SSD | 50 GB NVMe SSD |
| CUDA | 11.0+ | 11.8+ | 12.1+ |

---

## ðŸŽ“ Research & Development

### Algorithm Overview

**Motion Detection Pipeline:**
1. YOLO object detection (YOLOv8 architecture)
2. Bounding box tracking with Kalman filtering
3. Contact point analysis using intersection-over-union (IoU)
4. Optical flow computation (Farneback algorithm)
5. Frequency extraction via Fast Fourier Transform (FFT)
6. Signal smoothing (Savitzky-Golay filter, order 3, window 11)

**Dual-Axis Tracking:**
- **Primary Axis (L0)**: Vertical motion, range [0, 100]
- **Secondary Axis (L1)**: Rotation/lateral, range [-90Â°, +90Â°]

**Frequency Analysis:**
- Peak detection using `scipy.signal.find_peaks`
- Moving window FFT (window size: 60 frames)
- Amplitude normalization and outlier rejection

### Publications & Citations

If you use this framework in academic research, please cite:

```bibtex
@software{fungen_live_screen,
  title={FunGen Live Screen: Advanced AI-Powered Video Motion Analysis Framework},
  author={FunGen Team},
  year={2024},
  version={0.5.4},
  url={https://github.com/Jeremiah2077/fun-tool}
}
```

---

## ðŸ¤ Contributing

We welcome contributions from the research and developer community!

### Development Workflow

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/innovative-tracker`)
3. Implement your changes with tests
4. Ensure code passes linting (`flake8`, `black`)
5. Update documentation as needed
6. Submit a Pull Request with detailed description

### Code Style

- Follow PEP 8 guidelines
- Use type hints for function signatures
- Document all public APIs with docstrings
- Maintain test coverage above 80%

### Areas for Contribution

- ðŸ” New tracking algorithms
- ðŸ§© Signal processing plugins
- ðŸ“Š Visualization enhancements
- ðŸŽ¯ Performance optimizations
- ðŸ“š Documentation improvements
- ðŸ§ª Test coverage expansion

---

## ðŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

### Third-Party Licenses

- **OpenCV**: Apache 2.0 License
- **PyTorch**: BSD-style License
- **Ultralytics YOLO**: AGPL-3.0 License
- **SciPy**: BSD License

---

## ðŸ™ Acknowledgments

This framework builds upon cutting-edge research and open-source projects:

- **[Ultralytics YOLO](https://github.com/ultralytics/ultralytics)**: State-of-the-art object detection
- **[OpenCV](https://opencv.org/)**: Computer vision foundation
- **[PyTorch](https://pytorch.org/)**: Deep learning infrastructure
- **[SciPy](https://scipy.org/)**: Scientific computing tools
- **[MSS](https://github.com/BoboTiG/python-mss)**: Efficient screen capture

---

## ðŸ“§ Contact & Support

### Issues & Bug Reports
- **GitHub Issues**: [Report a bug](https://github.com/Jeremiah2077/fun-tool/issues)
- **Feature Requests**: [Suggest a feature](https://github.com/Jeremiah2077/fun-tool/issues/new?template=feature_request.md)

### Community
- **Discussions**: [GitHub Discussions](https://github.com/Jeremiah2077/fun-tool/discussions)
- **Discord**: [Join our server](#)

### Professional Support
For commercial licensing and enterprise support inquiries:
- Email: support@fungen-project.com

---

## ðŸ”’ Security & Privacy

### Data Processing
- **Local Processing**: All video analysis is performed locally on your machine
- **No Telemetry**: No usage data or video content is transmitted externally
- **Secure Storage**: SQLite databases use local filesystem encryption when available

### Responsible Use
This tool is designed for legitimate research, development, and analysis purposes. Users must:
- Comply with all applicable laws and regulations
- Respect privacy and consent requirements
- Use the software ethically and responsibly
- Not use for surveillance without proper authorization

---

## ðŸ“š Documentation

### Additional Resources
- **[User Guide](docs/user_guide.md)**: Comprehensive usage instructions
- **[API Reference](docs/api_reference.md)**: Detailed API documentation
- **[Plugin Development](funscript/user_plugins/PLUGIN_DEVELOPMENT_GUIDE.md)**: Plugin creation tutorial
- **[Tracker Development](docs/tracker_development.md)**: Custom tracker guide
- **[Performance Tuning](docs/performance.md)**: Optimization strategies

---

## ðŸŽ¯ Roadmap

### Version 0.6.0 (Q1 2025)
- [ ] Real-time multi-object tracking
- [ ] Enhanced VR format support
- [ ] Web-based visualization dashboard
- [ ] RESTful API for remote processing

### Version 0.7.0 (Q2 2025)
- [ ] TensorRT optimization for inference
- [ ] Transformer-based tracking models
- [ ] Cloud processing support
- [ ] Mobile app integration

### Long-term Goals
- [ ] Multi-camera synchronization
- [ ] 3D pose estimation integration
- [ ] Real-time streaming protocol
- [ ] Distributed processing framework

---

## ðŸ“Š Statistics

![GitHub stars](https://img.shields.io/github/stars/Jeremiah2077/fun-tool?style=social)
![GitHub forks](https://img.shields.io/github/forks/Jeremiah2077/fun-tool?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/Jeremiah2077/fun-tool?style=social)

---

**Built with precision. Powered by AI. Made for researchers and developers.**

*FunGen Live Screen v0.5.4 - Advanced Video Motion Intelligence*

---

## âš¡ Quick Links

- [Installation Guide](#installation)
- [Quick Start](#quick-start)
- [API Documentation](docs/api_reference.md)
- [Contributing Guidelines](#contributing)
- [License](#license)
- [Support](#contact--support)

---

*For educational, research, and development purposes. Use responsibly.*
